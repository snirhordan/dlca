{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Working with data in `PyTorch`\n",
    "<a id=part1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll learn about the `Dataset` and `DataLoader` classes which are part of `PyTorch`'s `torch.util.data` package.\n",
    "These are highly useful abstractions that can greatly reduce the amount of boilerplate code you need to write in order to work with data.\n",
    "Knowing how to use these classes properly will prove useful in the coming assignments and course project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "torch.random.manual_seed(42)\n",
    "test = unittest.TestCase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "<a id=part1_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` class is an abstraction over a sequence of python objects,\n",
    "each representing a sample (with or without a label). it's main purpose is\n",
    "to load a single (possibly labeled) sample from some soure (disk, web, etc) into memory,\n",
    "and transform it into a usuable representation (e.g. image to tensor).\n",
    "\n",
    "The `Dataset` abstracts away exactly when the data is loaded into memory: It can be on\n",
    "demand when each sample is accessed, all in advance or some combination using e.g. caching.\n",
    "This is implementation-specific.\n",
    "\n",
    "As a warm up, lets create a demonstration `Dataset` that returns noise images. It should:\n",
    "- Return tensors of shape `(C, W, H)` containing random contents.\n",
    "- Label each returned tensor with a class label, an integer between `0` and `num_classes-1`.\n",
    "- Initialize each returned tensor with a uniform distribution on `[0, 255]`.\n",
    "- Return a total of `num_samples` labeled images.\n",
    "- The same image should be returned every time the dataset is accessed as the same index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's implement a simple function to generate a labelled random image.\n",
    "\n",
    "**TODO** Implement the `random_labelled_image` function in the `hw1/datasets.py` module.\n",
    "Use the code below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Iterator\n",
    "from contextlib import contextmanager\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "\n",
    "\n",
    "def random_labelled_image(\n",
    "    shape: Tuple[int, ...], num_classes: int, low=0, high=255, dtype=torch.int,\n",
    ") -> Tuple[Tensor, int]:\n",
    "    \"\"\"\n",
    "    Generates a random image and a random class label for it.\n",
    "    :param shape: The shape of the generated image e.g. (C, H, W).\n",
    "    :param num_classes: Number of classes. The label should be in [0, num_classes-1].\n",
    "    :param low: Minimal value in the image (inclusive).\n",
    "    :param high: Maximal value in the image (exclusive).\n",
    "    :param dtype: Data type of the returned image tensor.\n",
    "    :return: A tuple containing the generated image tensor and it's label.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    #  Implement according to the docstring description.\n",
    "    # ====== YOUR CODE: ======\n",
    "    image = high*torch.rand(shape)\n",
    "    label = torch.randint(0,num_classes-1,(1,))\n",
    "    # ========================\n",
    "    return image, label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hw1.datasets as hw1datasets\n",
    "import cs236781.plot as plot\n",
    "\n",
    "image_shape = (3, 32, 64)\n",
    "num_classes = 3\n",
    "low, high = 0, 10\n",
    "\n",
    "# Generate some random images and check values\n",
    "X_ = None\n",
    "for i in range(100):\n",
    "    X, y = random_labelled_image(image_shape, num_classes, low, high)\n",
    "    test.assertEqual(X.shape, image_shape)\n",
    "    test.assertIsInstance(y, int)\n",
    "    test.assertTrue(0<= y < num_classes)\n",
    "    test.assertTrue(torch.all((X >= low) & (X < high)))\n",
    "    if X_ is not None:\n",
    "        test.assertFalse(torch.all(X == X_))\n",
    "    X_ = X\n",
    "    \n",
    "plot.tensors_as_images([X, X_]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases we'll need to consistently get repeatable results even though we're using pseudo-random number generators (PRNGs).\n",
    "The way to do this is to provide a seed to the generator.\n",
    "Given the same seed, a PRNG will always generate the same sequence of numbers.\n",
    "\n",
    "Here, we need a way to generate the same random image when accessing our dataset at the same index (e.g. to simulate a real set of images).\n",
    "\n",
    "**TODO** Implement the `torch_temporary_seed` function in the `hw1/datasets.py` module.\n",
    "Use the code below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@contextmanager\n",
    "def torch_temporary_seed(seed: int):\n",
    "    \"\"\"\n",
    "    A context manager which temporarily sets torch's random seed, then sets the random\n",
    "    number generator state back to its previous state.\n",
    "    :param seed: The temporary seed to set.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    #  Implement this context manager as described.\n",
    "    #  See torch.random.get/set_rng_state(), torch.random.manual_seed().\n",
    "    # ====== YOUR CODE: ======\n",
    "    state = torch.get_rng_state()\n",
    "    # ========================\n",
    "    try:\n",
    "        # ====== YOUR CODE: ======\n",
    "        torch.random.manual_seed(seed)\n",
    "        # ========================\n",
    "        yield\n",
    "    finally:\n",
    "        # ====== YOUR CODE: ======\n",
    "        torch.set_rng_state(state)\n",
    "        # ========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [42, 24]\n",
    "torch.random.manual_seed(seeds[0])\n",
    "\n",
    "# Before the context, the first seed affects the output\n",
    "data_pre_context = torch.randn(100,)\n",
    "\n",
    "with torch_temporary_seed(seeds[1]):\n",
    "    # Within this context, the second seed is in effect\n",
    "    data_in_context = torch.randn(100,)\n",
    "    \n",
    "# After the context, the random state should be restored\n",
    "data_post_context = torch.randn(100,)\n",
    "data_around_context = torch.cat([data_pre_context, data_post_context])\n",
    "\n",
    "# Use first seed, generate data in the same way but without changing context in the middle\n",
    "torch.random.manual_seed(seeds[0])\n",
    "data_no_context = torch.cat([torch.randn(100,), torch.randn(100,)])\n",
    "\n",
    "# Identical results show that the context didn't affect external random state\n",
    "test.assertTrue(torch.allclose(data_no_context, data_around_context))\n",
    "\n",
    "# The data generated in the context should match what we would generate with the second seed\n",
    "torch.random.manual_seed(seeds[1])\n",
    "test.assertTrue(torch.allclose(data_in_context, torch.randn(100,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the dataset as required.\n",
    "\n",
    "**TODO** Implement the `RandomImageDataset` class in the `hw1/datasets.py` module.\n",
    "Use the code below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:27.644164Z",
     "iopub.status.busy": "2022-03-12T13:39:27.644056Z",
     "iopub.status.idle": "2022-03-12T13:39:28.094684Z",
     "shell.execute_reply": "2022-03-12T13:39:28.094353Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test RandomImageDataset\n",
    "\n",
    "# Create the dataset\n",
    "num_samples = 500\n",
    "num_classes = 10\n",
    "image_size = (3, 32, 32)\n",
    "ds = hw1datasets.RandomImageDataset(num_samples, num_classes, *image_size)\n",
    "\n",
    "# You can load individual items from the dataset by indexing\n",
    "img0, cls0 = ds[139]\n",
    "\n",
    "# Plot first N images from the dataset with a helper function\n",
    "fig, axes = plot.dataset_first_n(ds, 9, show_classes=True, nrows=3)\n",
    "\n",
    "# The same image should be returned every time the same index is accessed\n",
    "for i in range(num_samples):\n",
    "    X, y = ds[i]\n",
    "    X_, y_ = ds[i]\n",
    "    test.assertEqual(X.shape, image_size)\n",
    "    test.assertIsInstance(y, int)\n",
    "    test.assertEqual(y, y_)\n",
    "    test.assertTrue(torch.all(X==X_))\n",
    "    \n",
    "# Should raise if out of range\n",
    "for i in range(num_samples, num_samples+10):\n",
    "    with test.assertRaises(ValueError):\n",
    "        ds[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple dataset is a useful abstraction when we know in advance the number of samples in our dataset and can access them by indexing. However, in many cases we simply cannot know about all data in advance. For example, perhaps new data is generated in real time.\n",
    "\n",
    "To deal with these cases, we can use a different type of abstraction: an `IterableDataset` which provides an interface only to iterate over samples, but not to index them directly.\n",
    "Let's implement such a dataset which will allow us to iterate over an infinite stream of randomly-generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:28.096671Z",
     "iopub.status.busy": "2022-03-12T13:39:28.096557Z",
     "iopub.status.idle": "2022-03-12T13:39:28.516600Z",
     "shell.execute_reply": "2022-03-12T13:39:28.516333Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = hw1datasets.ImageStreamDataset(num_classes, *image_size)\n",
    "\n",
    "# This dataset can't be indexed\n",
    "with test.assertRaises(NotImplementedError):\n",
    "    ds[0]\n",
    "    \n",
    "# There is no length\n",
    "with test.assertRaises(TypeError):\n",
    "    len(ds)\n",
    "    \n",
    "# Arbitrarily stop somewhere\n",
    "stop = torch.randint(2**11, 2**16, (1,)).item()\n",
    "    \n",
    "# We can iterate over it, indefinitely\n",
    "for i, (X, y) in enumerate(ds):\n",
    "    test.assertEqual(X.shape, image_size)\n",
    "    test.assertIsInstance(y, int)\n",
    "    \n",
    "    if i > stop:\n",
    "        break\n",
    "        \n",
    "print(f'Generated {i} images')\n",
    "test.assertGreater(i, stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Datasets and Transforms\n",
    "<a id=part1_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a simple `Dataset` to see how they work, we'll load one of `pytorch`'s built-in datasets: CIFAR-10. This is a famous dataset consisting of 60,000 small `32x32` color images classified into 10 classes. You can read more about it [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "The `torchvision` package has built-in `Dataset` classes that can download the data to a local folder,\n",
    "load it, transform it using arbitrary transform functions and iterate over the resulting samples.\n",
    "\n",
    "Run the following code block to download and create a CIFAR-10 `Dataset`. It won't be downloaded again if already present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to download CIFAR-10 and plot some random images from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:28.518513Z",
     "iopub.status.busy": "2022-03-12T13:39:28.518413Z",
     "iopub.status.idle": "2022-03-12T13:39:30.942241Z",
     "shell.execute_reply": "2022-03-12T13:39:30.941942Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as tvtf\n",
    "\n",
    "cfar10_labels = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "data_root = os.path.expanduser('~/.pytorch-datasets')\n",
    "\n",
    "cifar10_train_ds = torchvision.datasets.CIFAR10(\n",
    "    root=data_root, download=True, train=True,\n",
    "    transform=tvtf.ToTensor()\n",
    ")\n",
    "\n",
    "print('Number of samples:', len(cifar10_train_ds))\n",
    "\n",
    "# Plot them with a helper function\n",
    "fig, axes = plot.dataset_first_n(cifar10_train_ds, 64,\n",
    "                                 show_classes=True, class_labels=cfar10_labels,\n",
    "                                 nrows=8, hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the entire CIFAR-10 dataset, we would like to work with a smaller subset\n",
    "from it to reduce runtime of the code in this notebook.\n",
    "A simple way to achieve this with `Datasets` is to wrap a `Dataset` in another `Dataset` that does this for us. This will make it easy to use our subset with `DataLoader`s as you will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Complete the implementation of `SubsetDataset` in `hw1/datasets.py` and use the following code block to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:30.946039Z",
     "iopub.status.busy": "2022-03-12T13:39:30.945932Z",
     "iopub.status.idle": "2022-03-12T13:39:30.964890Z",
     "shell.execute_reply": "2022-03-12T13:39:30.964623Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subset_len = 5000\n",
    "subset_offset = 1234\n",
    "cifar10_train_subset_ds = hw1datasets.SubsetDataset(cifar10_train_ds, subset_len, subset_offset)\n",
    "\n",
    "dataset_x, dataset_y  = cifar10_train_ds[subset_offset + 10]\n",
    "subset_x, subset_y  = cifar10_train_subset_ds[10]\n",
    "\n",
    "# Tests\n",
    "test.assertEqual(len(cifar10_train_subset_ds), subset_len)\n",
    "test.assertTrue(torch.all(dataset_x == subset_x))\n",
    "test.assertEqual(dataset_y, subset_y)\n",
    "with test.assertRaises(IndexError, msg=\"Out of bounds index should raise IndexError\"):\n",
    "    tmp = cifar10_train_subset_ds[subset_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we initialized the `Dataset` instance for CIFAR-10, we provided a `transform` parameter.\n",
    "This is a way to specify an arbitrary transformation that should be run on each sample prior to returning it from the dataset.\n",
    "\n",
    "In the above, we used the `ToTensor()` transformation from `torchvision.transforms` to convert the\n",
    "images from a PIL (Python Imaging Library) image object which has a shape of `32x32x3` and values in range \\[0, 255\\] into a pytorch `Tensor` of shape `3x32x32` and values in range \\[0, 1\\].\n",
    "\n",
    "To demonstrate the use of transforms, we'll implement two custom transforms which invert the colors and flip the images around the horizontal axis.\n",
    "\n",
    "**TODO** Complete the `InvertColors` and `FlipUpDown` classes in the `hw1/transforms.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:30.966704Z",
     "iopub.status.busy": "2022-03-12T13:39:30.966608Z",
     "iopub.status.idle": "2022-03-12T13:39:33.910241Z",
     "shell.execute_reply": "2022-03-12T13:39:33.909965Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hw1.transforms as hw1transforms\n",
    "\n",
    "cifar10_inverted_ds = torchvision.datasets.CIFAR10(\n",
    "    root=data_root, download=True, train=True,\n",
    "    transform=tvtf.Compose([ # Compose allows us to chain multiple transforms in a sequence\n",
    "        tvtf.ToTensor(), # Convert PIL image to pytorch Tensor (C,H,W) in range [0,1]\n",
    "        hw1transforms.InvertColors(),\n",
    "        hw1transforms.FlipUpDown(),\n",
    "    ])\n",
    ")\n",
    "\n",
    "fig, axes = plot.dataset_first_n(cifar10_inverted_ds, 64,\n",
    "                                 show_classes=True, class_labels=cfar10_labels,\n",
    "                                 nrows=8, hspace=0.5)\n",
    "\n",
    "test.assertTrue(torch.allclose(cifar10_train_ds[0][0], torch.flip(1.-cifar10_inverted_ds[0][0], [1])),\n",
    "               \"Wrong custom transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader`s and `Sampler`s\n",
    "<a id=part1_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that a `Dataset` is simply an iterable allowing us to iterate over samples and posssible to also access them by index.\n",
    "Simple to implement, but not very powerful.\n",
    "The real benefit is when combining them with `DataLoader`.\n",
    "A `DataLoader` samples a batch of samples from the dataset according to logic defined by a `Sampler` object.\n",
    "The sampler decides how to partition the dataset into batches of `N` samples.\n",
    "The `DataLoader` additionally handles loading samples in parallel to speed up creation of a batch.\n",
    "\n",
    "A major motivation here is memory usage. When combining a `DataLoader` with a `Dataset` we can easily\n",
    "control memory constraints by simply setting the batch size.\n",
    "This is important since large datasets (e.g. ImageNet) do not fit in memory of most machines.\n",
    "Since a `Dataset` can lazily load samples from disk on access,\n",
    "and the `DataLoader` can sample random samples from it in parallel, we are provided with a simple\n",
    "yet high-performance mechanism to iterate over random batches from our dataset without needing to\n",
    "hold all of it in memory.\n",
    "\n",
    "Let's create a basic `DataLoader` for our CIFAR-10 dataset.\n",
    "Run the follwing code block multiple times and observe that different samples are shown each time in the first few batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:33.914088Z",
     "iopub.status.busy": "2022-03-12T13:39:33.913976Z",
     "iopub.status.idle": "2022-03-12T13:39:48.748832Z",
     "shell.execute_reply": "2022-03-12T13:39:48.748518Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple DataLoader that partitions the data into batches\n",
    "# of size N=8 in random order, using two background proceses\n",
    "cifar10_train_dl = torch.utils.data.DataLoader(\n",
    "    cifar10_train_ds, batch_size=8, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "# Iterate over batches sampled with our DataLoader\n",
    "num_batches_to_show = 5\n",
    "for idx, (images, classes) in enumerate(cifar10_train_dl):\n",
    "    # The DataLoader returns a tuple of:\n",
    "    # images: Tensor of size NxCxWxH\n",
    "    # classes: Tensor of size N\n",
    "    fig, axes = plot.tensors_as_images(images, figsize=(8, 1))\n",
    "    fig.suptitle(f'Batch #{idx+1}:', x=0, y=0.6)\n",
    "    if idx >= num_batches_to_show - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we specified `shuffle=True` to the `DataLoader`. This automatically created a `Sampler` which just returns indices from the `DataSet` in a random order.\n",
    "\n",
    "To better control the content of the batches, we can create our own custom sampler.\n",
    "Imagine we want each batch to contain one sample from the beginning of the dataset and\n",
    "another from the end. If we have `N` samples, we would like to get the following sequence of indices: \\[0, N-1, 1, N-2, 2, N-3, ...\\] and then use a`batch_size` of 2.\n",
    "\n",
    "**TODO** Implement the `FirstLastSampler` class in the `hw1/dataloaders.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:48.750868Z",
     "iopub.status.busy": "2022-03-12T13:39:48.750741Z",
     "iopub.status.idle": "2022-03-12T13:39:48.869765Z",
     "shell.execute_reply": "2022-03-12T13:39:48.869467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hw1.dataloaders as hw1dataloaders\n",
    "\n",
    "# Test sampler with odd number of elements\n",
    "sampler = hw1dataloaders.FirstLastSampler(list(range(5)))\n",
    "test.assertEqual(list(sampler), [0,4, 1,3, 2,])\n",
    "\n",
    "# Test sampler with evennumber of elements\n",
    "sampler = hw1dataloaders.FirstLastSampler(list(range(6)))\n",
    "test.assertEqual(list(sampler), [0,5, 1,4, 2,3])\n",
    "\n",
    "\n",
    "# Create a DataLoader that partitions the data into batches\n",
    "# of size N=2 in an order determined by our custom sampler\n",
    "cifar10_train_dl = torch.utils.data.DataLoader(\n",
    "    cifar10_train_ds, batch_size=2, num_workers=0,\n",
    "    sampler=hw1dataloaders.FirstLastSampler(cifar10_train_ds),\n",
    ")\n",
    "\n",
    "# Iterate over batches sampled with our DataLoader\n",
    "num_batches_to_show = 3\n",
    "for idx, (images, classes) in enumerate(cifar10_train_dl):\n",
    "    fig, axes = plot.tensors_as_images(images, figsize=(8, 1))\n",
    "    fig.suptitle(f'Batch #{idx+1}:', x=0, y=0.6)\n",
    "    if idx >= num_batches_to_show - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Test Sets\n",
    "<a id=part1_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know about `DataLoaders` we can use them to do something useful: split a training dataset into **Training and Validation** sets.\n",
    "\n",
    "A common issue in machine learning models is abundance of hyperparameters that must be selected prior to training the model on data. These hyperparameters may be part of the model itself or part of the training process. \n",
    "We would like to determine which hyperparameter selection can best fit the training data, and, more importantly, can be able to generalize to unseen data.\n",
    "\n",
    "A prevalent approach is therefore to split the training dataset into two parts:\n",
    "One for actual training, i.e. tuning model parameters e.g. weights in the case of neural nets,\n",
    "and another for validation, i.e. comparing one model or set of hyperparameters to another.\n",
    "After the best model is selected (by seeking the minimal validation error), it can be retrained with the entire training set.\n",
    "\n",
    "![img](https://cdn-images-1.medium.com/max/1600/1*Nv2NNALuokZEcV6hYEHdGA.png)\n",
    "\n",
    "**TODO** Implement the function `create_train_validation_loaders` in the `hw1/dataloaders.py` module.\n",
    "Use the following code block to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:48.871918Z",
     "iopub.status.busy": "2022-03-12T13:39:48.871806Z",
     "iopub.status.idle": "2022-03-12T13:39:48.901948Z",
     "shell.execute_reply": "2022-03-12T13:39:48.901649Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing the train/validation split dataloaders\n",
    "import hw1.dataloaders as hw1dataloaders\n",
    "\n",
    "validation_ratio = 0.2\n",
    "dl_train, dl_valid = hw1dataloaders.create_train_validation_loaders(cifar10_train_ds, validation_ratio)\n",
    "\n",
    "train_idx = set(dl_train.sampler.indices)\n",
    "valid_idx = set(dl_valid.sampler.indices)\n",
    "train_size = len(train_idx)\n",
    "valid_size = len(valid_idx)\n",
    "print('Training set size: ', train_size)\n",
    "print('Validation set size: ', valid_size)\n",
    "\n",
    "# Tests\n",
    "test.assertEqual(train_size+valid_size, len(cifar10_train_ds), \"Incorrect total number of samples\")\n",
    "test.assertEqual(valid_size, validation_ratio * (train_size + valid_size), \"Incorrect ratio\")\n",
    "test.assertTrue(train_idx.isdisjoint(valid_idx), \"Train and validation sets are not disjoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw1/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:48.903819Z",
     "iopub.status.busy": "2022-03-12T13:39:48.903691Z",
     "iopub.status.idle": "2022-03-12T13:39:48.922960Z",
     "shell.execute_reply": "2022-03-12T13:39:48.922620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw1.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Question 1\n",
    "\n",
    "Determine whether each of the following statements is **true or false**, and explain why in detail:\n",
    "\n",
    "1. The test set allows us to estimate our in-sample error.\n",
    "2. Any split of the data into two disjoint subsets would constitute an equally useful train-test split.\n",
    "3. The test-set should not be used during cross-validation.\n",
    "4. After performing cross-validation, we use the validation-set performance of each fold as a proxy for the model's generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:48.924958Z",
     "iopub.status.busy": "2022-03-12T13:39:48.924852Z",
     "iopub.status.idle": "2022-03-12T13:39:48.942037Z",
     "shell.execute_reply": "2022-03-12T13:39:48.941782Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part1_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Your friend has trained a simple linear regression model, e.g. $\\hat{y}=\\vectr{w}\\vec{x}+b$, with some training data. He then evaluated it on a disjoint test-set and concluded that the model has over-fit the training set and therefore decided to add a regularization term $\\lambda \\norm{\\vec{w}}^w$ to the loss, where $\\lambda$ is a hyper parameter.\n",
    "In order to select the value of $\\lambda$, your friend re-trained the model on his training set with different values of $\\lambda$ and then chose the value which produced the best results on the test set.\n",
    "\n",
    "Is your friend's approach justified? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T13:39:48.943744Z",
     "iopub.status.busy": "2022-03-12T13:39:48.943638Z",
     "iopub.status.idle": "2022-03-12T13:39:48.960190Z",
     "shell.execute_reply": "2022-03-12T13:39:48.959901Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part1_q2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "4c02d59ce3d54ca19b9a0581ac078c04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4cfb011275ef4d8fad5b2c250e797b05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "538d3c804f9f41eb9b187ff2e4a84965": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5782b58570274812a6ab8dc215df2544": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_538d3c804f9f41eb9b187ff2e4a84965",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_61ed4285264e4e719bb6794f0be877e1",
       "value": ""
      }
     },
     "61ed4285264e4e719bb6794f0be877e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "999d76a4871447f09d657dd91af5ec65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4cfb011275ef4d8fad5b2c250e797b05",
       "max": 170498071,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4c02d59ce3d54ca19b9a0581ac078c04",
       "value": 170498071
      }
     },
     "b51218540f904b9a80aa97af812329d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c138ed9a105247caa0bb3b3f0fdbc148": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c1bb1ee003934b62acf7a6c5fd957f2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b51218540f904b9a80aa97af812329d9",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_c138ed9a105247caa0bb3b3f0fdbc148",
       "value": " 170499072/? [00:41&lt;00:00, 8231266.93it/s]"
      }
     },
     "d62f0be15b174163830604c49a29e7aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5782b58570274812a6ab8dc215df2544",
        "IPY_MODEL_999d76a4871447f09d657dd91af5ec65",
        "IPY_MODEL_c1bb1ee003934b62acf7a6c5fd957f2b"
       ],
       "layout": "IPY_MODEL_fe42f74d53cd46f289065ace5ad07190"
      }
     },
     "fe42f74d53cd46f289065ace5ad07190": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
